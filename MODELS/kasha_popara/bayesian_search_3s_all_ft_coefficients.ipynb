{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Dense, Dropout\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def metrics_report(self, y_test: np.ndarray, y_pred: np.ndarray):\n",
    "\n",
    "        y_test = np.ravel(y_test)\n",
    "        y_pred = np.ravel(y_pred)\n",
    "\n",
    "        true_positives = np.sum(y_test * y_pred)\n",
    "        false_positives = np.sum(np.abs(y_test - 1) * y_pred)\n",
    "        true_negatives = np.sum((y_test - 1) * (y_pred - 1))\n",
    "        false_negatives = np.sum(y_test * np.abs(y_pred - 1))\n",
    "\n",
    "        accuracy = round((true_positives + true_negatives) / (true_positives + true_negatives + false_positives + false_negatives), 4)\n",
    "        precision = round(true_positives / (true_positives + false_positives), 4)\n",
    "        recall = round(true_positives / (true_positives + false_negatives), 4)\n",
    "        specificity = round(true_negatives / (true_negatives + false_positives), 4)\n",
    "        npv = round(true_negatives / (true_negatives + false_negatives), 4)\n",
    "        f1_1 = round(2 * (precision * recall) / (precision + recall), 4)\n",
    "        f1_0 = round(2 * (specificity * npv) / (specificity + npv), 4)\n",
    "        f1_macro = round((f1_1 + f1_0) / 2, 4)\n",
    "\n",
    "        scores_dict = dict(\n",
    "            accuracy = accuracy, f1_macro = f1_macro,\n",
    "            f1_1 = f1_1, f1_0 = f1_0,\n",
    "            Precision = precision, Recall = recall,\n",
    "            Specificity = specificity, npv = npv,\n",
    "            TP = int(true_positives), FP = int(false_positives), FN = int(false_negatives),\n",
    "            TN = int(true_negatives),\n",
    "            y_test_shape = y_test.shape,\n",
    "            y_pred_shape = y_pred.shape,\n",
    "            total_samples = true_negatives + true_positives + false_positives + false_negatives,\n",
    "        )\n",
    "\n",
    "        return scores_dict\n",
    "\n",
    "\n",
    "def create_model():\n",
    "\n",
    "    self.model.add(Dense(units = layer_dict['units'],\n",
    "                         input_shape = (self.split_sets['X_train'].shape[1],),\n",
    "                         activation = layer_dict['activation'],\n",
    "                         kernel_initializer = self.hypermethods['initializer'],\n",
    "                         kernel_regularizer = self.hypermethods['regularizer']))\n",
    "\n",
    "    self.model.add(Dense(units = layer_dict['units'],\n",
    "                         activation = layer_dict['activation'],\n",
    "                         kernel_initializer = self.hypermethods['initializer'],\n",
    "                         kernel_regularizer = self.hypermethods['regularizer']))\n",
    "\n",
    "    self.model.add(Dropout(rate = layer_dict['rate']))\n",
    "\n",
    "    self.model.compile(loss = self.hypermethods['loss'],\n",
    "                       optimizer = self.hypermethods['optimizer'],\n",
    "                       metrics = [\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    print(self.model.summary())\n",
    "\n",
    "\n",
    "X_train = pd.read_csv(f'{PROJECT_DIR}/DATA\\MODELS_DATA\\AFDB_CUDB_MITDB\\Ann\\AnnSegments_Clean\\AFvsNoAF_Oversampled_AB_B_T\\Splits\\\\{SPLIT_ID}/features_{SEGMENT_LENGTH.lower()}_train.csv')\n",
    "X_test = pd.read_csv(f'{PROJECT_DIR}/DATA\\MODELS_DATA\\AFDB_CUDB_MITDB\\Ann\\AnnSegments_Clean\\AFvsNoAF_Oversampled_AB_B_T\\Splits\\\\{SPLIT_ID}/features_{SEGMENT_LENGTH.lower()}_test.csv')\n",
    "y_train = X_train.pop('AF Clean Class')\n",
    "y_test = X_test.pop('AF Clean Class')\n",
    "\n",
    "episodes_train = X_train.pop('Clean Class')\n",
    "episodes_test = X_test.pop('Clean Class')\n",
    "\n",
    "# Data selection\n",
    "\n",
    "beat_cols = [col for col in X_train.columns if re.match(r'Beat\\d', col)]\n",
    "rr_cols = [col for col in X_train.columns if re.match(r'RR\\d', col)]\n",
    "rrbpm_cols = [col for col in X_train.columns if re.match(r'RRBPM\\d', col)]\n",
    "\n",
    "if RRS_ID == 'RR':\n",
    "    X_train.drop(rrbpm_cols, inplace = True, axis = 1)\n",
    "    X_test.drop(rrbpm_cols, inplace = True, axis = 1)\n",
    "\n",
    "if RRS_ID == 'RRBPM':\n",
    "    X_train.drop(rr_cols, inplace = True, axis = 1)\n",
    "    X_test.drop(rr_cols, inplace = True, axis = 1)\n",
    "\n",
    "if BEATS_ID == 'NB':\n",
    "    X_train.drop(beat_cols, inplace = True, axis = 1)\n",
    "    X_test.drop(beat_cols, inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "search_space = dict(\n",
    "        layers_list = [\n",
    "            [\n",
    "                {'units': int(X_train.shape[1]), 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': int(X_train.shape[1] / 2), 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "            [\n",
    "                {'units': int(X_train.shape[1]), 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': int(X_train.shape[1] * 2 / 3), 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': int(X_train.shape[1] * 1 / 3), 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "            [\n",
    "                {'units': int(X_train.shape[1] * 2), 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': int(X_train.shape[1]), 'type': 'dense', 'activation': 'relu'},\n",
    "                {'units': int(X_train.shape[1] / 2), 'type': 'dense', 'activation': 'relu'},\n",
    "            ],\n",
    "        ],\n",
    "        batch_size = [10000,5000,1000],\n",
    "        learning_rate = [0.2, 0.1, 0.03],\n",
    "        optimizer = ['Adam', 'Nadam', 'RMSProp'],\n",
    "        loss = ['CategoricalHinge', 'CategoricalCrossentropy', 'Poisson'],\n",
    "        regularizer = ['tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01)', None],\n",
    "        initializer = ['VarianceScaling', 'GlorotNormal'],\n",
    "        patience = [500],\n",
    "        decay = [1e-2],\n",
    "        early_stopping_flag = [True],\n",
    "        validation_split = [0.05],\n",
    "        epochs = [5],\n",
    ")\n",
    "\n",
    "# combinations\n",
    "keys, values = zip(*search_space.items())\n",
    "combinations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "# tabular combinations\n",
    "model_names = [f'{EXPERIMENT_ID}_{i+1}' for i in range(len(combinations_dicts))]\n",
    "combinations_df = pd.DataFrame(combinations_dicts)\n",
    "combinations_df.insert(0, 'Model', model_names)\n",
    "combinations_df['Trained'] = 'No'\n",
    "\n",
    "# save\n",
    "search_space_path = f'{metadata_search_space_dir.to_string}\\\\search_space_{EXPERIMENT_ID}.xlsx'\n",
    "combinations_df.to_excel(search_space_path, index = False)\n",
    "\n",
    "\n",
    "\n",
    "self.callbacks.append(keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss', mode = 'min', verbose = 10,\n",
    "    patience = callbacks_dict_of_dicts['early_stopping']['patience']\n",
    "))\n",
    "\n",
    "            self.callbacks.append(\n",
    "                keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "                                              mode = 'min',\n",
    "                                              verbose = 10,\n",
    "                                              patience = self.callbacks_dict_of_dicts['early_stopping']['patience'])\n",
    "            )\n",
    "\n",
    "\n",
    "with self.comet_experiment.train():\n",
    "\n",
    "    fit_metadata_tracker = self.model.fit(x = self.split_sets['X_train'],\n",
    "                                          y = self.split_sets['y_train'],\n",
    "                                          epochs = self.hyperparameters['epochs'],\n",
    "                                          batch_size = self.hyperparameters['batch_size'],\n",
    "                                          validation_split = self.hyperparameters['validation_split'],\n",
    "                                          callbacks = self.callbacks,\n",
    "                                          verbose = 10,\n",
    "                                          class_weight = self.split_sets_metadata['class_weight'],\n",
    "                                          workers = -1\n",
    "                                          )\n",
    "\n",
    "return self.metrics_report(y_pred = self.model.predict_classes(self.split_sets['X_test']), y_test = self.split_sets['y_test'])\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}